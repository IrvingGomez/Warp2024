{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install imageio[ffmpeg]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "if False:\n",
        "    import gymnasium as gym\n",
        "    from gymnasium.utils.play import play\n",
        "\n",
        "    play(\n",
        "        gym.make(\n",
        "            \"FrozenLake-v1\",\n",
        "            map_name=\"8x8\",\n",
        "            is_slippery=False,\n",
        "            render_mode=\"rgb_array\"\n",
        "        ),\n",
        "        keys_to_action={'w': 3, 'a': 0, 'd': 2, 's':1},\n",
        "        noop=2\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_FROZENLAKE_4X4 = True\n",
        "RECORD_VIDEO_FROZENLAKE_4X4 = True\n",
        "\n",
        "TRAIN_TAXI = True\n",
        "RECORD_VIDEO_TAXI = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njb_ProuHiOe"
      },
      "source": [
        "# Q-Learning with FrozenLake-v1 ‚õÑ and Taxi-v3\n",
        "\n",
        "In this notebook, **you'll code your first Reinforcement Learning agent from scratch** to play FrozenLake ‚ùÑÔ∏è using Q-Learning, play and experiment with different configurations.\n",
        "\n",
        "‚¨áÔ∏è Here is an example of what **you will achieve in just a couple of minutes.** ‚¨áÔ∏è\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRU_vXBrl1Jx"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" alt=\"Environments\" width=600px/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V68VveLacfxJ"
      },
      "source": [
        "*Q-Learning* **is the RL algorithm that**:\n",
        "\n",
        "- Trains *Q-Function*, an **action-value function** that is encoded, in internal memory, by a *Q-table* **that contains all the state-action pair values.**\n",
        "\n",
        "- Given a state and action, our Q-Function **will search the Q-table for the corresponding value.**\n",
        "    \n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q function\"  width=\"60%\"/>\n",
        "\n",
        "- When the training is done, **we have an optimal Q-Function, so an optimal Q-Table.**\n",
        "    \n",
        "- And if we **have an optimal Q-function**, we\n",
        "have an optimal policy, since we **know for, each state, the best action to take.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\"  width=\"60%\"/>\n",
        "\n",
        "\n",
        "But, in the beginning,¬†our **Q-Table is useless since it gives arbitrary value for each state-action pair¬†(most of the time we initialize the Q-Table to 0 values)**. But, as we‚Äôll¬†explore the environment and update our Q-Table it will give us better and better approximations\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"60%\"/>\n",
        "\n",
        "This is the Q-Learning pseudocode:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"60%\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEtx8Y8MqKfH"
      },
      "source": [
        "## Part 1: Frozen Lake ‚õÑ (non slippery version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAvihuHdy9tw"
      },
      "source": [
        "### Create and understand [FrozenLake environment ‚õÑ](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
        "\n",
        "A good habit when you start to use an environment is to check its documentation\n",
        "\n",
        "üëâ https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
        "\n",
        "---\n",
        "\n",
        "We're going to train our Q-Learning agent **to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)**.\n",
        "\n",
        "We can have two sizes of environment:\n",
        "\n",
        "- `map_name=\"4x4\"`: a 4x4 grid version\n",
        "- `map_name=\"8x8\"`: a 8x8 grid version\n",
        "\n",
        "\n",
        "The environment has two modes:\n",
        "\n",
        "- `is_slippery=False`: The agent always moves **in the intended direction** due to the non-slippery nature of the frozen lake (deterministic).\n",
        "- `is_slippery=True`: The agent **may not always move in the intended direction** due to the slippery nature of the frozen lake (stochastic)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaW_LHfS0PY2"
      },
      "source": [
        "For now let's keep it simple with the 4x4 map and non-slippery.\n",
        "We add a parameter called `render_mode` that specifies how the environment should be visualised. In our case because we **want to record a video of the environment at the end, we need to set render_mode to rgb_array**.\n",
        "\n",
        "As [explained in the documentation](https://gymnasium.farama.org/api/env/#gymnasium.Env.render) ‚Äúrgb_array‚Äù: Return a single frame representing the current state of the environment. A frame is a np.ndarray with shape (x, y, 3) representing RGB values for an x-by-y pixel image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jNxUbPMP0akP"
      },
      "outputs": [],
      "source": [
        "EnvFrozenLake4X4 = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KASNViqL4tZn"
      },
      "source": [
        "You can create your own custom grid like this:\n",
        "\n",
        "```python\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
        "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
        "```\n",
        "\n",
        "but we'll use the default environment for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXbTfdeJ1Xi9"
      },
      "source": [
        "#### Let's see what the Environment looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZNPG0g_UGCfh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_____OBSERVATION SPACE_____ \n",
            "\n",
            "Observation Space Discrete(16)\n",
            "Sample observation 8\n"
          ]
        }
      ],
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space\", EnvFrozenLake4X4.observation_space)\n",
        "print(\"Sample observation\", EnvFrozenLake4X4.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "We see with `Observation Space Shape Discrete(+64)` that the observation is an integer representing the **agent‚Äôs current position as current_row * ncols + current_col (where both the row and col start at 0)**.\n",
        "\n",
        "For example, the goal position in the 8x8 map can be calculated as follows: 7 * 8 + 7 = 63. The number of possible observations is dependent on the size of the map. **For example, the 8x8 map has 64 possible observations.**\n",
        "\n",
        "For instance, this is what state = 0 looks like:\n",
        "\n",
        "<img src=\"FrozenLakeInitial.png\" alt=\"FrozenLake\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "We5WqOBGLoSm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " _____ACTION SPACE_____ \n",
            "\n",
            "Action Space Shape 4\n",
            "Action Space Sample 2\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", EnvFrozenLake4X4.action_space.n)\n",
        "print(\"Action Space Sample\", EnvFrozenLake4X4.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyxXwkI2Magx"
      },
      "source": [
        "The action space is discrete with 4 actions available üéÆ:\n",
        "- 0: Go left\n",
        "- 1: Go down\n",
        "- 2: Go right\n",
        "- 3: Go up\n",
        "\n",
        "Reward function üí∞:\n",
        "- Reach goal: +1\n",
        "- Reach hole: 0\n",
        "- Reach frozen: 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pFhWblk3Awr"
      },
      "source": [
        "### Create and initialize the Q-table üóÑÔ∏è\n",
        "\n",
        "(Step 1 of the pseudocode)\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"60%\"/>\n",
        "\n",
        "It's time to initialize our Q-table! To know how many rows (states) and columns (actions) to use, we need to know the action and observation space. We already know their values from before, but we'll want to obtain them programmatically so that our algorithm generalizes for different environments. `Gym` provides us a way to do that: `env.action_space.n` and `env.observation_space.n`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "HuTKv3th3ohG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are  16  possible states\n",
            "There are  4  possible actions\n"
          ]
        }
      ],
      "source": [
        "StateSpace = EnvFrozenLake4X4.observation_space.n\n",
        "print(\"There are \", StateSpace, \" possible states\")\n",
        "\n",
        "ActionSpace = EnvFrozenLake4X4.action_space.n\n",
        "print(\"There are \", ActionSpace, \" possible actions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lnrb_nX33fJo"
      },
      "outputs": [],
      "source": [
        "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atll4Z774gri"
      },
      "source": [
        "### Define the greedy policy ü§ñ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "se2OzWGW5kYJ"
      },
      "outputs": [],
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation: take the action with the highest state, action value\n",
        "  action = np.argmax(Qtable[state][:])\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flILKhBU3yZ7"
      },
      "source": [
        "### Define the epsilon-greedy policy ü§ñ\n",
        "\n",
        "(Step 2 of the pseudocode)\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Epsilon-greedy is the training policy that handles the exploration/exploitation trade-off.\n",
        "\n",
        "The idea with epsilon-greedy:\n",
        "\n",
        "- With probability $1-\\varepsilon$ : **we do exploitation** (i.e. our agent selects the action with the highest state-action pair value).\n",
        "\n",
        "- With probability $\\varepsilon$: **we do exploration** (trying a random action).\n",
        "\n",
        "As the training continues, we progressively **reduce the epsilon value since we will need less and less exploration and more exploitation.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-Learning\" width=\"60%\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "cYxHuckr4LiG"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(Qtable, env, state, epsilon):\n",
        "  # Randomly generate a number between 0 and 1\n",
        "  random_num = random.uniform(0,1)\n",
        "  # if random_num > greater than epsilon --> exploitation\n",
        "  if random_num > epsilon:\n",
        "    # Take the action with the highest value given a state\n",
        "    # np.argmax can be useful here\n",
        "    action = greedy_policy(Qtable, state)\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDb7Tdx8atfL"
      },
      "source": [
        "### Create the training loop method\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"60%\"/>\n",
        "\n",
        "<img src=\"Q-learning-8.png\" alt=\"Q-Learning-formula\" width=\"60%\">\n",
        "\n",
        "The training loop goes like this:\n",
        "\n",
        "```\n",
        "For episode in the total of training episodes:\n",
        "\n",
        "Reduce epsilon (since we need less and less exploration)\n",
        "Reset the environment\n",
        "\n",
        "  For step in max timesteps:    \n",
        "    Choose the action At using epsilon greedy policy\n",
        "    Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "    Update the Q-value Q(s,a) using Bellman equation Q(s,a) + lr [R(s,a) + gamma * max_a' Q(s',a') - Q(s,a)]\n",
        "    If done, finish the episode\n",
        "    Our next state is the new state\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "IyZaYbUAeolw"
      },
      "outputs": [],
      "source": [
        "def train(n_training_episodes, max_epsilon, min_epsilon, discount_rate_g, learning_rate_a, decay_rate, env, Qtable):\n",
        "  for episode in range(n_training_episodes):\n",
        "    # Reset the environment\n",
        "    state, info = env.reset()\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    epsilon = max_epsilon\n",
        "\n",
        "    while not terminated or not truncated:\n",
        "      # Choose the action At using epsilon greedy policy\n",
        "      action = epsilon_greedy_policy(Qtable, env, state, epsilon)\n",
        "\n",
        "      # Take action At and observe Rt+1 and St+1\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate_a * (reward + discount_rate_g * np.max(Qtable[new_state]) - Qtable[state][action])\n",
        "\n",
        "      # Our next state is the new state\n",
        "      state = new_state\n",
        "\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    #epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    epsilon = max(epsilon - decay_rate, min_epsilon)\n",
        "\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set the hyperparameters ‚öôÔ∏è\n",
        "\n",
        "The exploration related hyperparamters are some of the most important ones.\n",
        "\n",
        "- We need to make sure that our agent **explores enough of the state space** to learn a good value approximation. To do that, we need to have progressive decay of the epsilon.\n",
        "- If you decrease epsilon too fast (too high decay_rate), **you take the risk that your agent will be stuck**, since your agent didn't explore enough of the state space and hence can't solve the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "N_TRAINING_EPISODES = 1000  # Total training episodes\n",
        "LEARNING_RATE_A = 0.9        # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "N_EVAL_EPISODES = 100        # Total number of test episodes\n",
        "EVAL_SEED = []               # The evaluation seed of the environment\n",
        "\n",
        "# Environment parameters\n",
        "ENV_ID = \"FrozenLake-v1-4X4\"     # Name of the environment\n",
        "MAX_STEPS = 200              # Max steps per episode, I use this parameter for the evaluation step\n",
        "DISCOUNT_RATE_G = 0.9        # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "MAX_EPSILON = 1              # Exploration probability at start\n",
        "MIN_EPSILON = 0              # Minimum exploration probability\n",
        "DECAY_RATE  = 0.0001         # Decay rate for exploration prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwKQ4tUdhGI"
      },
      "source": [
        "### Train the Q-Learning agent or load an already trained Q-Learning agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "DPBxfjJdTCOH"
      },
      "outputs": [],
      "source": [
        "if TRAIN_FROZENLAKE_4X4:\n",
        "    Qtable_frozenlake = initialize_q_table(StateSpace, ActionSpace)\n",
        "    Qtable_frozenlake = train(\n",
        "        N_TRAINING_EPISODES,\n",
        "        MAX_EPSILON,\n",
        "        MIN_EPSILON,\n",
        "        DISCOUNT_RATE_G,\n",
        "        LEARNING_RATE_A,\n",
        "        DECAY_RATE,\n",
        "        EnvFrozenLake4X4,\n",
        "        Qtable_frozenlake\n",
        "        )\n",
        "    np.savetxt(\"Qtable_frozenlake_4X4.csv\", Qtable_frozenlake, delimiter=\",\")\n",
        "else:\n",
        "    Qtable_frozenlake = np.loadtxt(\"Qtable_frozenlake_4X4.csv\", delimiter=\",\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Record a video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Qo57HBn3W74O"
      },
      "outputs": [],
      "source": [
        "def record_video(env, Qtable, out_directory, max_steps, random_policy=False, fps=1):\n",
        "  \"\"\"\n",
        "  Generate a replay video of the agent\n",
        "  :param env\n",
        "  :param Qtable: Qtable of our agent\n",
        "  :param out_directory\n",
        "  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n",
        "  \"\"\"\n",
        "  images = []\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "\n",
        "  state, info = env.reset(seed=random.randint(0,500))\n",
        "  img = env.render()\n",
        "  images.append(img)\n",
        "\n",
        "  for step in range(max_steps):\n",
        "    if random_policy:\n",
        "      action = np.random.choice(range(env.action_space.n))\n",
        "    else:\n",
        "      # Take the action (index) that have the maximum expected future reward given that state\n",
        "      action = np.argmax(Qtable[state][:])\n",
        "\n",
        "    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n",
        "    img = env.render()\n",
        "    images.append(img)\n",
        "\n",
        "    if terminated or truncated:\n",
        "      break\n",
        "\n",
        "  img = env.render()\n",
        "  images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RECORD_VIDEO_FROZENLAKE_4X4:\n",
        "    record_video(EnvFrozenLake4X4, Qtable_frozenlake, ENV_ID + \"-OptimalPolicy.mp4\", MAX_STEPS, fps=1)\n",
        "    record_video(EnvFrozenLake4X4, Qtable_frozenlake, ENV_ID + \"-RandomPolicy.mp4\", MAX_STEPS, random_policy=True, fps=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18lN8Bz7yvLt"
      },
      "source": [
        "## Part 2: Taxi-v3\n",
        "\n",
        "### Create and understand [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
        "A good habit when you start to use an environment is to check its documentation\n",
        "\n",
        "üëâ https://gymnasium.farama.org/environments/toy_text/taxi/\n",
        "\n",
        "---\n",
        "\n",
        "In `Taxi-v3`, there are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue).\n",
        "\n",
        "When the episode starts, **the taxi starts off at a random square** and the passenger is at a random location. The taxi drives to the passenger‚Äôs location, **picks up the passenger**, drives to the passenger‚Äôs destination (another one of the four specified locations), and then **drops off the passenger**. Once the passenger is dropped off, the episode ends.\n",
        "\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "gL0wpeO8gpej"
      },
      "outputs": [],
      "source": [
        "Env_taxi = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBOaXgtsrmtT"
      },
      "source": [
        "There are **500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger** (including the case when the passenger is in the taxi), and **4 destination locations.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_TPNaGSZrgqA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are  500  possible states\n"
          ]
        }
      ],
      "source": [
        "StateSpace = Env_taxi.observation_space.n\n",
        "print(\"There are \", StateSpace, \" possible states\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "CdeeZuokrhit"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are  6  possible actions\n"
          ]
        }
      ],
      "source": [
        "ActionSpace = Env_taxi.action_space.n\n",
        "print(\"There are \", ActionSpace, \" possible actions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1r50Advrh5Q"
      },
      "source": [
        "The action space (the set of possible actions the agent can take) is discrete with **6 actions available üéÆ**:\n",
        "\n",
        "- 0: move south\n",
        "- 1: move north\n",
        "- 2: move east\n",
        "- 3: move west\n",
        "- 4: pickup passenger\n",
        "- 5: drop off passenger\n",
        "\n",
        "Reward function üí∞:\n",
        "\n",
        "- -1 per step unless other reward is triggered.\n",
        "- +20 delivering passenger.\n",
        "- -10 executing ‚Äúpickup‚Äù and ‚Äúdrop-off‚Äù actions illegally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUMKPH0_LJyH"
      },
      "source": [
        "### Define the hyperparameters ‚öôÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "N_TRAINING_EPISODES = 25000  # Total training episodes\n",
        "LEARNING_RATE_A = 0.7        # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "N_EVAL_EPISODES = 100        # Total number of test episodes\n",
        "EVAL_SEED = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n",
        " 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n",
        " 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148]\n",
        "\n",
        "# Environment parameters\n",
        "ENV_ID = \"Taxi-v3\"           # Name of the environment\n",
        "MAX_STEPS = 99               # Max steps per episode, I use this parameter for the evaluation step\n",
        "DISCOUNT_RATE_G = 0.9        # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "MAX_EPSILON = 1              # Exploration probability at start\n",
        "MIN_EPSILON = 0              # Minimum exploration probability\n",
        "DECAY_RATE  = 0.001          # Decay rate for exploration prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train our Q-Learning agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_TAXI:\n",
        "    Qtable_taxi = initialize_q_table(StateSpace, ActionSpace)\n",
        "    Qtable_taxi = train(N_TRAINING_EPISODES, MAX_EPSILON, MIN_EPSILON, DISCOUNT_RATE_G, LEARNING_RATE_A, DECAY_RATE, Env_taxi, Qtable_taxi)\n",
        "    np.savetxt(\"Qtable_taxi.csv\", Qtable_taxi, delimiter=\",\")\n",
        "else:\n",
        "    Qtable_taxi = np.loadtxt(\"Qtable_taxi.csv\", delimiter=\",\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Record a video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RECORD_VIDEO_TAXI:\n",
        "    record_video(Env_taxi, Qtable_taxi, ENV_ID + \"-OptimalPolicy.mp4\", MAX_STEPS, fps=1)\n",
        "    record_video(Env_taxi, Qtable_taxi, ENV_ID + \"-RandomPolicy.mp4\", MAX_STEPS, random_policy=True, fps=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "67OdoKL63eDD",
        "B2_-8b8z5k54",
        "8R5ej1fS4P2V",
        "Pnpk2ePoem3r"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
