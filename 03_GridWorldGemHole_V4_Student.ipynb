{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_STATE = ['normal', 'stone', 'gem', 'hole']\n",
    "ACTIONS = ['up', 'down', 'left', 'right']\n",
    "ACTIONSYMBOLS = {'up': \"↑\", 'down': \"↓\", 'left': \"←\", 'right': \"→\", '': ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnvRobot = np.full((3,4), 'normal')\n",
    "\n",
    "EnvRobot[0,3] = 'gem'\n",
    "EnvRobot[1,1] = 'stone'\n",
    "EnvRobot[1,3] = 'hole'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EnvRobot.png](EnvRobot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Dynamic of environment\n",
    "\n",
    "We determine the next state and reward given the current state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NextStateAndReward(Position, Action, Rewards={'tired':0, 'stone':0, 'gem':1, 'hole':-1}, Environment=EnvRobot):\n",
    "\n",
    "    Worldsize = Environment.shape\n",
    "\n",
    "    # We select the next position according to the action taken\n",
    "    if Action == 'up':\n",
    "        NextPosition = (Position[0]-1, Position[1])\n",
    "    elif Action == 'down':\n",
    "        NextPosition = (Position[0]+1, Position[1])\n",
    "    elif Action == 'left':\n",
    "        NextPosition = (Position[0], Position[1]-1)\n",
    "    else:\n",
    "        NextPosition = (Position[0], Position[1]+1)\n",
    "\n",
    "    # We check if the next state is inside the grid world, if it is not, then we stay in the same state\n",
    "    if(\n",
    "        NextPosition[0] >= 0\n",
    "        and NextPosition[0] <= Worldsize[0] -1\n",
    "        and NextPosition[1] >= 0\n",
    "        and NextPosition[1] <= Worldsize[1] -1\n",
    "        ):\n",
    "        # We check the type of state that is the next state\n",
    "        if Environment[NextPosition] == 'normal':\n",
    "            Reward = Rewards['tired']\n",
    "        elif Environment[NextPosition] == 'stone':\n",
    "            Reward = Rewards['stone']\n",
    "            NextPosition = Position\n",
    "        elif Environment[NextPosition] == 'gem':\n",
    "            Reward = Rewards['gem']\n",
    "        else:\n",
    "            Reward = Rewards['hole']\n",
    "    else:\n",
    "        NextPosition = Position\n",
    "        Reward = Rewards['tired']\n",
    "    \n",
    "    return (NextPosition, Reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Stochastic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is slippery floor, the next action is stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SlipperyOpposite = {\n",
    "    'up': {'up':0.8, 'down':0.2, 'left':0, 'right':0},\n",
    "    'down': {'up':0.2, 'down':0.8, 'left':0, 'right':0},\n",
    "    'left': {'up':0, 'down':0, 'left':0.8, 'right':0.2},\n",
    "    'right': {'up':0, 'down':0, 'left':0.2, 'right':0.8}\n",
    "}\n",
    "\n",
    "SlipperyRandom = {\n",
    "}\n",
    "\n",
    "SlipperyRandomNotOpposite = {\n",
    "}\n",
    "\n",
    "SlipperyCompletelyRandom = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Iterative state value evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyValueEvaluation(\n",
    "        Policy,\n",
    "        Environment = EnvRobot,\n",
    "        Rewards={'tired':0, 'stone':0, 'gem':1, 'hole':-1},\n",
    "        DiscountRate=0.9,\n",
    "        NumberIterations=100,\n",
    "        DeterministicPolicy=True,\n",
    "        Slippery=False,\n",
    "        SlipperyDistribution=None,\n",
    "        Story=False\n",
    "    ):\n",
    "    # If Slippery is True, we have to pass the SlipperyDistribution\n",
    "\n",
    "    Worldsize = Environment.shape\n",
    "\n",
    "    StateValue = np.zeros(Worldsize)\n",
    "\n",
    "    for _ in range(NumberIterations):\n",
    "        NewStateValue = np.zeros(Worldsize)\n",
    "        for i in range(Worldsize[0]):\n",
    "            for j in range(Worldsize[1]):\n",
    "                if Environment[i,j] == 'normal':\n",
    "                    if DeterministicPolicy:\n",
    "                        # Deterministic Policy\n",
    "                        if not Slippery:\n",
    "                            # Deterministic Environment\n",
    "                            Action = Policy((i,j))\n",
    "                            NextPosition, Reward = NextStateAndReward((i,j), Action, Rewards, Environment)\n",
    "                            NewStateValue[i,j] += Reward + DiscountRate * StateValue[NextPosition]\n",
    "                        else:\n",
    "                            # Stochastic Environment\n",
    "                            Action = Policy((i,j))\n",
    "                            # We have to take into account the distribution of possible actions\n",
    "                            for ActualAction in SlipperyDistribution[Action]:\n",
    "                                ProbActualAction = SlipperyDistribution[Action][ActualAction]\n",
    "                                NextPosition, Reward = NextStateAndReward((i,j), ActualAction, Rewards, Environment)\n",
    "                                NewStateValue[i,j] += ProbActualAction * (Reward + DiscountRate * StateValue[NextPosition])\n",
    "                    #else:\n",
    "                        # Stochastic Policy\n",
    "                        #if not Slippery:\n",
    "                            # Deterministic Environment\n",
    "                            #for Action in :\n",
    "                                #ProbAction =\n",
    "                                #NextPosition, Reward =\n",
    "                                #NewStateValue[i,j] +=\n",
    "                        #else:\n",
    "                            # Stochastic Environment\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "        StateValue = np.copy(NewStateValue)\n",
    "\n",
    "        if Story:\n",
    "            print(StateValue.round(2))\n",
    "            print(  )\n",
    "\n",
    "    return(StateValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotPolicyValue(Policy, Value, DeterministicPolicy=True):\n",
    "    if DeterministicPolicy:\n",
    "        sns.heatmap(\n",
    "        Value,\n",
    "        annot=np.vectorize(ACTIONSYMBOLS.get)(Policy),\n",
    "        fmt = '',\n",
    "        cmap='RdBu',\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        linewidths=0.7,\n",
    "        linecolor=\"black\",\n",
    "        xticklabels=[],\n",
    "        yticklabels=[],\n",
    "        square=True\n",
    "    )\n",
    "    else:\n",
    "        sns.heatmap(\n",
    "        Value,\n",
    "        cmap='RdBu',\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        linewidths=0.7,\n",
    "        linecolor=\"black\",\n",
    "        xticklabels=[],\n",
    "        yticklabels=[],\n",
    "        square=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Iterative state-action value evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertStateActionToPositionTable(s, a, Environment=EnvRobot):\n",
    "    Worldsize = Environment.shape\n",
    "    return (s[0]*Worldsize[1] + s[1], ACTIONS.index(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyActionEvaluation(\n",
    "        Policy,\n",
    "        Environment = EnvRobot,\n",
    "        Rewards={'tired':0, 'stone':0, 'gem':1, 'hole':-1},\n",
    "        DiscountRate=0.9,\n",
    "        NumberIterations=100,\n",
    "        DeterministicPolicy=True,\n",
    "        Slippery=False,\n",
    "        SlipperyDistribution=None,\n",
    "        Story=False\n",
    "    ):\n",
    "    # If Slippery is True, we have to give the SlipperyDistribution also\n",
    "\n",
    "    Worldsize = \n",
    "\n",
    "    QTable = \n",
    "\n",
    "    for _ in range(NumberIterations):\n",
    "        NewQTable = \n",
    "        for i in range(Worldsize[0]):\n",
    "            for j in range(Worldsize[1]):\n",
    "                if Environment[i,j] == 'normal':\n",
    "                    if DeterministicPolicy:\n",
    "                        # Deterministic Policy\n",
    "                        if not Slippery:\n",
    "                            # Deterministic Environment\n",
    "                            Action = \n",
    "                            NextPosition, Reward = \n",
    "\n",
    "                            Aux = 0\n",
    "                            if Environment[NextPosition] == 'normal':\n",
    "                                ActionPrime = \n",
    "                                Aux += \n",
    "\n",
    "                            NewQTable[] +=\n",
    "                        else:\n",
    "                            # Stochastic Environment\n",
    "                            Action = \n",
    "                            # We have to take into account the distribution of possible actions\n",
    "                            for ActualAction in :\n",
    "                                ProbActualAction = \n",
    "                                NextPosition, Reward = \n",
    "\n",
    "                                Aux = 0\n",
    "                                if Environment[NextPosition] == 'normal':\n",
    "                                    ActionPrime = \n",
    "                                    Aux += \n",
    "\n",
    "                                NewQTable[] += \n",
    "                    else:\n",
    "                        # Stochastic Policy\n",
    "                        if not Slippery:\n",
    "                            # Deterministic Environment\n",
    "                            for Action in :\n",
    "                                NextPosition, Reward = \n",
    "\n",
    "                                Aux = 0\n",
    "                                if Environment[NextPosition] == 'normal':\n",
    "                                    for ActionPrime in :\n",
    "                                        ProbActionPrime = \n",
    "                                        Aux += \n",
    "\n",
    "                                NewQTable[] += \n",
    "                        else:\n",
    "                            # Stochastic Environment\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "        QTable = np.copy(NewQTable)\n",
    "\n",
    "        if Story:\n",
    "            print(QTable.round(2))\n",
    "            print(  )\n",
    "\n",
    "    return(QTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Iterative policy improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GreedyPolicy(QTable, Environment=EnvRobot):\n",
    "    Worldsize = Environment.shape\n",
    "\n",
    "    Policy = []\n",
    "\n",
    "    for s in range(len(QTable)):\n",
    "        if np.all(np.isclose(QTable[s], QTable[s][0])):\n",
    "            Policy.append(random.choice(ACTIONS))\n",
    "        else:\n",
    "            Policy.append(ACTIONS[np.argmax(QTable[s])])\n",
    "\n",
    "    Policy = np.array(Policy).reshape((Worldsize[0],Worldsize[1]))\n",
    "\n",
    "    for i in range(Worldsize[0]):\n",
    "        for j in range(Worldsize[1]):\n",
    "            if Environment[i,j] != 'normal':\n",
    "                Policy[i,j] = ''\n",
    "\n",
    "    return Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValueFromQTable(QTable, Policy, Environment=EnvRobot, DeterministicPolicy=True):\n",
    "    Worldsize = Environment.shape\n",
    "\n",
    "    StateValue = np.zeros(Worldsize)\n",
    "    \n",
    "    for i in range(Worldsize[0]):\n",
    "        for j in range(Worldsize[1]):\n",
    "            if Environment[i,j] == 'normal':\n",
    "                if DeterministicPolicy:\n",
    "                    # Deterministic Policy\n",
    "                    Action =\n",
    "                    StateValue[i,j] =\n",
    "                else:\n",
    "                    # Stochastic Policy\n",
    "                    for Action in :\n",
    "                        ProbAction = \n",
    "                        StateValue[i,j] += \n",
    "\n",
    "    return StateValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyIterativeEstimation(\n",
    "        Policy,\n",
    "        Environment = EnvRobot,\n",
    "        Rewards={'tired':0, 'stone':0, 'gem':1, 'hole':-1},\n",
    "        DiscountRate=0.9,\n",
    "        NumberIterations=100,\n",
    "        DeterministicPolicy=True,\n",
    "        Slippery=False,\n",
    "        SlipperyDistribution=None,\n",
    "        Story=False\n",
    "    ):\n",
    "    for _ in range(NumberIterations):\n",
    "        # Policy Evaluation\n",
    "        QTable = \n",
    "\n",
    "        ValuePolicy = \n",
    "\n",
    "        # Policy Improvement\n",
    "        PolicyArray = \n",
    "        \n",
    "        if not DeterministicPolicy:\n",
    "            DeterministicPolicy = True\n",
    "\n",
    "        def Policy(Position):\n",
    "            return PolicyArray[Position]\n",
    "\n",
    "        if Story:\n",
    "            print(\"QTable:\\n\")\n",
    "            print(QTable.round(2))\n",
    "            print(  )\n",
    "\n",
    "            print(\"State Value:\\n\")\n",
    "            print(ValuePolicy.round(2))\n",
    "            print(  )\n",
    "\n",
    "            print(\"k:{}\\n\".format(_+1))\n",
    "            print(\"Policy:\\n\")\n",
    "            print(np.vectorize(ACTIONSYMBOLS.get)(PolicyArray))\n",
    "            print(  )\n",
    "\n",
    "    return PolicyArray, ValuePolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Deterministic policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy1_array = np.array([\n",
    "    ['right', 'right', 'right', ''],\n",
    "    ['up', '', 'up', ''],\n",
    "    ['up', 'right', 'up', 'left']\n",
    "])\n",
    "\n",
    "def Policy1(Position):\n",
    "    return Policy1_array[Position]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Policy1.png](Policy1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy2_array = np.array([\n",
    "    ['right', 'right', 'right', ''],\n",
    "    ['up', '', 'right', ''],\n",
    "    ['right', 'right', 'right', 'up']\n",
    "])\n",
    "\n",
    "def Policy2(Position):\n",
    "    return Policy2_array[Position]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Policy2.png](Policy2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy3_array = np.array([\n",
    "    ['right', 'right', 'left', ''],\n",
    "    ['down', '', 'right', ''],\n",
    "    ['up', 'left', 'up', 'left']\n",
    "])\n",
    "\n",
    "def Policy3(Position):\n",
    "    return Policy3_array[Position]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Policy3.png](Policy3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Stochastic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Policy4(Position, Environment=EnvRobot):\n",
    "    Distribution = {'up':0.25, 'down':0.25, 'left':0.25, 'right':0.25}\n",
    "\n",
    "    if Environment[Position] == 'normal':\n",
    "        return Distribution\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Policy4.png](Policy4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 State value function evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Deterministic policy and deterministic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Policy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolicyValueEvaluation(\n",
    "    Policy=Policy1,\n",
    "    NumberIterations=5,\n",
    "    Story=True\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValuePolicy1 = PolicyValueEvaluation(Policy=Policy1)\n",
    "ValuePolicy1.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPolicyValue(Policy1_array, ValuePolicy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Policy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolicyValueEvaluation(\n",
    "    Policy=Policy2,\n",
    "    NumberIterations=5,\n",
    "    Story=True\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValuePolicy2 = PolicyValueEvaluation(Policy=Policy2)\n",
    "ValuePolicy2.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPolicyValue(Policy2_array, ValuePolicy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Policy 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolicyValueEvaluation(\n",
    "    Policy=Policy3,\n",
    "    NumberIterations=5,\n",
    "    Story=True\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValuePolicy3 = PolicyValueEvaluation(Policy=Policy3)\n",
    "ValuePolicy3.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPolicyValue(Policy3_array, ValuePolicy3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Deterministic policy and stochastic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Policy 1, Opposite slippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolicyValueEvaluation(\n",
    "    Policy=Policy1,\n",
    "    NumberIterations=5,\n",
    "    Slippery=True,\n",
    "    SlipperyDistribution=SlipperyOpposite,\n",
    "    Story=True\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueSlipperyPolicy1 = PolicyValueEvaluation(\n",
    "    Policy=Policy1,\n",
    "    Slippery=True,\n",
    "    SlipperyDistribution=SlipperyOpposite\n",
    ")\n",
    "\n",
    "ValueSlipperyPolicy1.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPolicyValue(Policy1_array, ValueSlipperyPolicy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Policy 2, Opposite slippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolicyValueEvaluation(\n",
    "    Policy=Policy2,\n",
    "    NumberIterations=5,\n",
    "    Slippery=True,\n",
    "    SlipperyDistribution=SlipperyOpposite,\n",
    "    Story=True\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueSlipperyPolicy2 = PolicyValueEvaluation(\n",
    "    Policy=Policy2,\n",
    "    Slippery=True,\n",
    "    SlipperyDistribution=SlipperyOpposite\n",
    ")\n",
    "\n",
    "ValueSlipperyPolicy2.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPolicyValue(Policy2_array, ValueSlipperyPolicy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Policy 3, Opposite slippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolicyValueEvaluation(\n",
    "    Policy=Policy3,\n",
    "    NumberIterations=5,\n",
    "    Slippery=True,\n",
    "    SlipperyDistribution=SlipperyOpposite,\n",
    "    Story=True\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueSlipperyPolicy3 = PolicyValueEvaluation(\n",
    "    Policy=Policy3,\n",
    "    Slippery=True,\n",
    "    SlipperyDistribution=SlipperyOpposite\n",
    ")\n",
    "\n",
    "ValueSlipperyPolicy3.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPolicyValue(Policy3_array, ValueSlipperyPolicy3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Stochastic policy and deterministic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Policy 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolicyValueEvaluation(\n",
    "    \n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValuePolicy4 = PolicyValueEvaluation()\n",
    "\n",
    "ValuePolicy4.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPolicyValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Stochastic policy and stochastic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Policy 4, Opposite slippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolicyValueEvaluation(\n",
    "    \n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueSlipperyPolicy4 = PolicyValueEvaluation(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPolicyValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 State-action value function evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Deterministic policies and deterministic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Policy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTablePolicy1 = PolicyActionEvaluation(Policy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTablePolicy1.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueFromQTable(QTablePolicy1, Policy1).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Policy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTablePolicy2 = PolicyActionEvaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTablePolicy2.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueFromQTable().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Policy 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTablePolicy3 = PolicyActionEvaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTablePolicy3.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueFromQTable().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Deterministic policy and stochastic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Policy 1, Opposite slippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTableSlipperyPolicy1 = PolicyActionEvaluation(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTableSlipperyPolicy1.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueFromQTable().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Policy 2, Opposite slippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTableSlipperyPolicy2 = PolicyActionEvaluation(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTableSlipperyPolicy2.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueFromQTable().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Policy 3, Opposite slippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTableSlipperyPolicy3 = PolicyActionEvaluation(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTableSlipperyPolicy3.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueFromQTable().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Stochastic policy and deterministic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Policy 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTablePolicy4 = PolicyActionEvaluation(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTablePolicy4.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueFromQTable().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Stochastic policy and stochastic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Policy 4, Opposite slippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTableSlipperyPolicy4 = PolicyActionEvaluation(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTableSlipperyPolicy4.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValueFromQTable().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Iterative policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Starting from Policy 2, by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vectorize(ACTIONSYMBOLS.get)(Policy2_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTable2_0 = PolicyActionEvaluation()\n",
    "ValuePolicy2_0 = ValueFromQTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTable2_0.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValuePolicy2_0.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy2_1_array = GreedyPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Policy2_1(Position):\n",
    "    return Policy2_1_array[Position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy2_1_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vectorize(ACTIONSYMBOLS.get)(Policy2_1_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTable2_1 = PolicyActionEvaluation()\n",
    "ValuePolicy2_1 = ValueFromQTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTable2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValuePolicy2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy2_2_array = GreedyPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Policy2_2(Position):\n",
    "    return Policy2_2_array[Position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy2_2_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vectorize(ACTIONSYMBOLS.get)(Policy2_2_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTable2_2 = PolicyActionEvaluation()\n",
    "ValuePolicy2_2 = ValueFromQTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTable2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValuePolicy2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Starting from Policy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolicyStar, ValueStar = PolicyIterativeEstimation(Policy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPolicyValue(PolicyStar, ValueStar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Starting from Policy 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PolicyStar, ValueStar = PolicyIterativeEstimation(Policy4, DeterministicPolicy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotPolicyValue(PolicyStar, ValueStar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
