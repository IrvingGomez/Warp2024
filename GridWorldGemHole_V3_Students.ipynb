{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_STATE = ['normal', 'stone', 'gem', 'hole']\n",
    "ACTIONS = ['up', 'down', 'left', 'right']\n",
    "ACTIONSYMBOLS = {'up': \"↑\", 'down': \"↓\", 'left': \"←\", 'right': \"→\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnvRobot = np.full((3,4), 'normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EnvRobot.png](EnvRobot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic of the environment\n",
    "\n",
    "We determine the next state and reward given the current state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NextStateAndReward(Position, Action, Rewards={'Tired':0, 'Stone':0, 'Gem':1, 'Hole':-1}, Environment=EnvRobot):\n",
    "\n",
    "    Worldsize = Environment.shape\n",
    "\n",
    "    # We select the next position according to the action taken\n",
    "    if Action == 'up':\n",
    "        NextPosition = (Position[0]-1, Position[1])\n",
    "    elif Action == 'down':\n",
    "        NextPosition = (Position[0]+1, Position[1])\n",
    "    elif Action == 'left':\n",
    "        NextPosition = (Position[0], Position[1]-1)\n",
    "    else:\n",
    "        NextPosition = (Position[0], Position[1]+1)\n",
    "\n",
    "    # We check if the next state is inside the grid world, if it is not, then we stay in the same state\n",
    "    if(\n",
    "        NextPosition[0] >= 0\n",
    "        and NextPosition[0] <= Worldsize[0] -1\n",
    "        and NextPosition[1] >= 0\n",
    "        and NextPosition[1] <= Worldsize[1] -1\n",
    "        ):\n",
    "        # We check the type of state that is the next state\n",
    "        if Environment[NextPosition] == 'normal':\n",
    "            Reward = \n",
    "        elif Environment[NextPosition] == 'stone':\n",
    "            Reward = \n",
    "            NextPosition = Position\n",
    "        elif Environment[NextPosition] == 'gem':\n",
    "            Reward = \n",
    "        else:\n",
    "            Reward = \n",
    "    else:\n",
    "        NextPosition = Position\n",
    "        Reward = \n",
    "    \n",
    "    return (NextPosition, Reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slippery dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is slippery floor, the next action is stochastic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SlipperyOpposite = {\n",
    "    'up': {},\n",
    "    'down': {},\n",
    "    'left': {},\n",
    "    'right': {}\n",
    "}\n",
    "\n",
    "SlipperyRandom = {\n",
    "}\n",
    "\n",
    "SlipperyRandomNotOpposite = {\n",
    "}\n",
    "\n",
    "SlipperyCompletelyRandom = {\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyEvaluation(\n",
    "        Policy,\n",
    "        Environment = EnvRobot,\n",
    "        Rewards={'Tired':0, 'Stone':0, 'Gem':1, 'Hole':-1},\n",
    "        DiscountRate=0.9,\n",
    "        NumberIterations=100,\n",
    "        DeterministicPolicy=True,\n",
    "        Slippery=False,\n",
    "        SlipperyDistribution=None,\n",
    "        Story=False\n",
    "    ):\n",
    "    # If Slippery is True, we have to pass the SlipperyDistribution\n",
    "\n",
    "    Worldsize = Environment.shape\n",
    "\n",
    "    StateValue = np.zeros(Worldsize)\n",
    "\n",
    "    for _ in range(NumberIterations):\n",
    "        NewStateValue = np.zeros(Worldsize)\n",
    "        for i in range(Worldsize[0]):\n",
    "            for j in range(Worldsize[1]):\n",
    "                if Environment[i,j] == 'normal':\n",
    "                    if DeterministicPolicy:\n",
    "                        # Deterministic Policy\n",
    "                        if not Slippery:\n",
    "                            # Deterministic Environment\n",
    "                            Action = \n",
    "                            NextPosition, Reward = \n",
    "                            NewStateValue[i,j] += \n",
    "                        else:\n",
    "                            # Stochastic Environment\n",
    "                            Action = \n",
    "                            # We have to take into account the distribution of possible actions\n",
    "                            for ActualAction in :\n",
    "                                ProbActualAction = \n",
    "                                NextPosition, Reward = \n",
    "                                NewStateValue[i,j] += \n",
    "                    else:\n",
    "                        # Stochastic Policy\n",
    "                        if not Slippery:\n",
    "                            # Deterministic Environment\n",
    "                            for Action in :\n",
    "                                ProbAction = \n",
    "                                NextPosition, Reward = \n",
    "                                NewStateValue[i,j] += \n",
    "                        else:\n",
    "                            # Stochastic Environment\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "        StateValue = np.copy(NewStateValue)\n",
    "\n",
    "        if Story:\n",
    "            print(StateValue.round(2))\n",
    "            print(  )\n",
    "\n",
    "    return(StateValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative action-value evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertStateActionToPositionTable(s, a, Environment=EnvRobot):\n",
    "    Worldsize = Environment.shape\n",
    "    return (s[0]*Worldsize[1] + s[1], ACTIONS.index(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyActionEvaluation(\n",
    "    ):\n",
    "    # If Slippery is True, we have to give the SlipperyDistribution also\n",
    "\n",
    "    Worldsize = \n",
    "\n",
    "    QTable = \n",
    "\n",
    "    for _ in range(NumberIterations):\n",
    "        NewQTable = \n",
    "        for i in range(Worldsize[0]):\n",
    "            for j in range(Worldsize[1]):\n",
    "                if Environment[i,j] == 'normal':\n",
    "                    if DeterministicPolicy:\n",
    "                        # Deterministic Policy\n",
    "                        if not Slippery:\n",
    "                            # Deterministic Environment\n",
    "                            Action = \n",
    "                            NextPosition, Reward = \n",
    "\n",
    "                            Aux = 0\n",
    "                            if Environment[NextPosition] == 'normal':\n",
    "                                ActionPrime = \n",
    "                                Aux += \n",
    "\n",
    "                            NewQTable[] += \n",
    "                        else:\n",
    "                            # Stochastic Environment\n",
    "                            Action = \n",
    "                            # We have to take into account the distribution of possible actions\n",
    "                            for ActualAction in :\n",
    "                                ProbActualAction = \n",
    "                                NextPosition, Reward = \n",
    "\n",
    "                                Aux = 0\n",
    "                                if Environment[NextPosition] == 'normal':\n",
    "                                    ActionPrime = \n",
    "                                    Aux += QTable[]\n",
    "\n",
    "                                NewQTable[] += \n",
    "                    else:\n",
    "                        # Stochastic Policy\n",
    "                        if not Slippery:\n",
    "                            # Deterministic Environment\n",
    "                            for Action in :\n",
    "                                NextPosition, Reward = \n",
    "\n",
    "                                Aux = 0\n",
    "                                if Environment[NextPosition] == 'normal':\n",
    "                                    for ActionPrime in :\n",
    "                                        ProbActionPrime = \n",
    "                                        Aux += \n",
    "\n",
    "                                NewQTable[] += \n",
    "                        else:\n",
    "                            # Stochastic Environment\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "        QTable = np.copy(NewQTable)\n",
    "\n",
    "        if Story:\n",
    "            print(QTable.round(2))\n",
    "            print(  )\n",
    "\n",
    "    return(QTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyExtraction(QTable, Environment=EnvRobot):\n",
    "    Worldsize = Environment.shape\n",
    "\n",
    "    Policy = np.array([ACTIONSYMBOLS[ACTIONS[_]] for _ in np.argmax(QTable, axis=1)]).reshape((Worldsize[0],Worldsize[1]))\n",
    "\n",
    "    for i in range(Worldsize[0]):\n",
    "        for j in range(Worldsize[1]):\n",
    "            if Environment[i,j] != 'normal':\n",
    "                Policy[i,j] = ''\n",
    "\n",
    "    return Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Policy1(Position):\n",
    "    # Position must be a 2-tuple, i.e. (i,j)\n",
    "    _ = np.array([\n",
    "        \n",
    "    ])\n",
    "\n",
    "    return _[Position]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Policy1.png](Policy1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Policy2(Position):\n",
    "    # Position must be a 2-tuple, i.e. (i,j)\n",
    "    _ = np.array([\n",
    "        \n",
    "    ])\n",
    "\n",
    "    return _[Position]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Policy2.png](Policy2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Policy3(Position):\n",
    "    # Position must be a 2-tuple, i.e. (i,j)\n",
    "    _ = np.array([\n",
    "        \n",
    "    ])\n",
    "\n",
    "    return _[Position]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Policy3.png](Policy3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Policy3(Position):\n",
    "    # Position must be a 2-tuple, i.e. (i,j)\n",
    "    _ = np.array([\n",
    "        \n",
    "    ])\n",
    "\n",
    "    return _[Position]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Policy4(Position, Environment=EnvRobot):\n",
    "    # Position must be a 2-tuple, i.e. (i,j)\n",
    "    Distribution = {}\n",
    "\n",
    "    if Environment[Position] == 'normal':\n",
    "        return Distribution\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Policy4.png](Policy4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic policy and deterministic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic policy and stochastic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy 1, Opposite slippery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy 2, Opposite slippery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy 3, Opposite slippery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic policy and deterministic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic policy and stochastic environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy 4, Opposite slippery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action-value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot Environment, no slippery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.81],\n",
       "       [0.  , 0.  , 0.  , 0.9 ],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.73, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.9 , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.66, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.73],\n",
       "       [0.81, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.73, 0.  ]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PolicyActionEvaluation(Policy1).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  ,  0.81],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.9 ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  1.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.73,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , -1.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , -0.73],\n",
       "       [ 0.  ,  0.  ,  0.  , -0.81],\n",
       "       [ 0.  ,  0.  ,  0.  , -0.9 ],\n",
       "       [-1.  ,  0.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PolicyActionEvaluation(Policy2).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTablePolicy4 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['→', '→', '→', ''],\n",
       "       ['↑', '', '↑', ''],\n",
       "       ['↑', '←', '←', '←']], dtype='<U1')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolicyExtraction(QTablePolicy4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
